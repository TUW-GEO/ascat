#+title: Working with ASCAT swath files
#+PROPERTY: header-args:ipython :results raw drawer :session tutorial
* Working with swath files
** Creating a SwathFileCollection
If we have a collection of time-series swath files, we can create a SwathFileCollection object that will handle them as a group.

#+begin_src ipython
from datetime import datetime
from importlib import reload

import ascat.read_native.ragged_array_ts as rat
reload(rat)

from time import time
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src ipython
swath_source = "/home/charriso/p14/data-write/RADAR/hsaf/h121_v1.0"
#+end_src

#+RESULTS:
:results:
:end:

It's important to understand the structure of the data, so that the SwathFileCollection can find and parse all of the data that is requested from it. Handily, this package comes with existing parsers for several ASCAT products. These can be used with ~SwathFileCollection.from_product_id()~:

#+begin_src ipython
collection = rat.SwathFileCollection.from_product_id(swath_source, "H121_V1.0")
#+end_src

#+RESULTS:
:results:
:end:

The currently included project ids are the keys of `ascat.read_native.xarray_io.swath_io_catalog`

#+begin_src ipython
from ascat.read_native.xarray_io import swath_io_catalog
swath_io_catalog.keys()
#+end_src

#+RESULTS:
:results:
: dict_keys(['H129', 'H129_V1.0', 'H121_V1.0', 'H122', 'SIG0_6.25', 'SIG0_12.5'])
:end:

If your data structure does not match anything included in the package, you can write a class inheriting from ~ascat.read_native.xarray_io.SwathIOBase~:

#+begin_src ipython :eval no
class AscatH121v1Swath(SwathIOBase):
    # outlines the basic structure of the swath filenames, with {bracketed} variable names
    # in the place of anything that changes according to the data inside
    fn_pattern = "W_IT-HSAF-ROME,SAT,SSM-ASCAT-METOP{sat}-12.5km-H121_C_LIIB_{placeholder}_{placeholder1}_{date}____.nc"

    # defines the names of the subfolder/directory names that contain the data. In this case,
    # files are sorted into folders by satellite and year, so that a typical filepath looks like
    # ".../metop_b/2021/W_IT-HSAF-ROME..."
    sf_pattern = {
        "satellite_folder": "metop_[abc]",
        "year_folder": "{year}"
    }

    # specifies the string format of the date that occupies the "date" field in fn_pattern
    date_format = "%Y%m%d%H%M%S"

    # provides the grid that the data is based on. In this case, using the grid_cache allows
    # us to reuse the same grid for multiple classes in the source code without
    # actually generating several different FibGrids.
    # however, it would also work to just write, for example:
    #     grid = FibGrid(12.5)
    grid = grid_cache.fetch_or_store("Fib12.5", FibGrid, 12.5)["grid"]

    # specifies the size (in degrees) of the grid cell to be written out by SwathFileCollection.stack()
    grid_cell_size = 5

    # specifies the filename format for the cells written out by SwathFileCollection.stack()
    cell_fn_format = "{:04d}.nc"

    # names any dataset variables with a "beams" dimension - not relevant with this particular product
    beams_vars = []

    # defines all the dataset's data variable names and the dtype that they should be written as when packed.
    ts_dtype = np.dtype([
        ("sat_id", np.int8),
        ("as_des_pass", np.int8),
        ("swath_indicator", np.int8),
        ("surface_soil_moisture", np.float32),
        ("surface_soil_moisture_noise", np.float32),
        ("backscatter40", np.float32),
        ("slope40", np.float32),
        ("curvature40", np.float32),
        ("surface_soil_moisture_sensitivity", np.float32),
        ("backscatter_flag", np.uint8),
        ("correction_flag", np.uint8),
        ("processing_flag", np.uint8),
        ("surface_flag", np.uint8),
        ("snow_cover_probability", np.int8),
        ("frozen_soil_probability", np.int8),
        ("wetland_fraction", np.int8),
        ("topographic_complexity", np.int8),
        ("subsurface_scattering_probability", np.int8),
    ])

    # a function for generating a filename regex for a particular timestamp
    # (used for filtering swath files by date)
    @staticmethod
    def fn_read_fmt(timestamp):
        return {"date": timestamp.strftime("%Y%m%d*"),
                "sat": "[ABC]",
                "placeholder": "*",
                "placeholder1": "*"}

    # a function that returns subfolder regexes based on a timestamp
    # Here, in the case of satellite_folder, we don't actually need to be able to search
    # by satellite, so we just have a regex that is inclusive of all three possible satellites.
    # However, in the case of year_folder, we want the search year to actually match with the
    # timestamp we pass to the function.
    @staticmethod
    def sf_read_fmt(timestamp):
        return {
            "satellite_folder": {"satellite": "metop_[abc]"},
            "year_folder": {"year": f"{timestamp.year}"},
        }

    def __init__(self, filename, **kwargs):
        super().__init__(filename, "netcdf4", **kwargs)
#+end_src

After creating your io class, you can use it to make a collection by passing it to the SwathFileCollection class:

#+begin_src ipython :eval no
collection = rat.SwathFileCollection(swath_source, ioclass=ASCATH121v1Swath)
#+end_src

Regardless of how you define you define your collection, once created it can be used to read data from your swath collection for any given date range and geographic extent. It can also be used to stack data in the collection into cellwise timeseries in indexed ragged array format, according to the CellGrid defined in the ioclass.

** Reading data from a SwathFileCollection
Let's start with a rectangular, roughly country-sized area in Central Europe, and a time period of four days. The collection has an ioclass, that ioclass has a grid, and that grid has a method `get_bbox_grid_points` that will take our boundaries and return the grid points within them. The bounding box reader isn't actually in the package yet but it will just do this under the hood.

#+begin_src ipython
bounds = (45, 50, 10, 20) #latmin, latmx, lonmin, lonmax
dates = (datetime(2020, 12, 1), datetime(2020, 12, 5))
gpis = collection.ioclass.grid.get_bbox_grid_points(latmin=45, latmax=50, lonmin=10, lonmax=20)
gpis
#+end_src

#+RESULTS:
:results:
#+BEGIN_EXAMPLE
  masked_array(data=[1162873, 1162928, 1163017, ..., 1260256, 1260345,
                     1260434],
               mask=False,
         fill_value=999999,
              dtype=int32)
#+END_EXAMPLE
:end:


Now that we have the gpis within our bounding box, we can pass them to the ~location_id~ argument of `collection.read()`.

#+begin_src ipython
# output = collection.read(location_id=gpis, date_range=dates)
output = collection.read(bbox=bounds, date_range=dates)
# output.load()
#+end_src

#+RESULTS:
:results:
:end:

Now we have a nice xarray dataset that we can work with however we wish. In this case, it's one-dimensional, so we are basically working with a tabular data structure.

First, to make sure we got the desired data, let's make some plots.

This is a somewhat silly plot but it shows that the data covers the time range we requested, and that it includes all three satellites.

#+begin_src ipython
%matplotlib inline
from matplotlib import pyplot as plt
import matplotlib.dates as mdates
plt.close()
fig, ax = plt.subplots()
scatter = ax.scatter(output.time, output.longitude, s=0.01, c=output.sat_id, cmap="rainbow", alpha=0.8)
legend1 = ax.legend(*scatter.legend_elements(), title="Satellite")
for i in range(3):
    legend1.get_texts()[i].set_text(f"Metop {chr(65+i)}")
ax.add_artist(legend1)
plt.xlabel("Time")
plt.ylabel("Latitude (degrees)")
ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m-%d\n%H:%M"))
plt.xticks(rotation=30)
plt.tight_layout()
#+end_src

#+RESULTS:
:results:
[[file:./obipy-resources/fg3R3K.png]]
:end:


We can check the spatial coverage of the data by plotting it on a map.

#+begin_src ipython
import cartopy.crs as ccrs

plt.close()
ax = plt.axes(projection=ccrs.PlateCarree())
ax.coastlines()
ax.gridlines(draw_labels=True)
ax.set_extent([-10, 30, 35, 65])
plt.scatter(
    output.longitude,
    output.latitude,
    s=0.1,
    alpha=0.1
)
#+end_src

#+RESULTS:
:results:
: <matplotlib.collections.PathCollection at 0x7f7c36f00d00>
[[file:./obipy-resources/pVGn0v.png]]
:end:

We requested the

Having the data as an xarray makes it handy to do transformations. For example, we can groupby location_id and get the average surface soil moisture at each.

#+begin_src ipython
# first we need to load the variable we'll be grouping by into memory
# currently it exists as a chunked dask array, which xarray apparently can't use
# as a
output["location_id"].load()
#+end_src

#+RESULTS:
:results:
#+BEGIN_EXAMPLE
  <xarray.DataArray 'location_id' (obs: 45007)>
  array([1258693., 1259680., 1260057., ..., 1260222., 1260311., 1260455.])
  Coordinates:
      latitude   (obs) float64 49.91 49.96 49.98 49.82 ... 49.98 49.99 49.99 50.0
      longitude  (obs) float64 19.94 19.78 19.35 19.84 ... 11.69 10.57 12.38 11.26
      time       (obs) datetime64[ns] 2020-12-01T08:09:59.415000064 ... 2020-12...
  Dimensions without coordinates: obs
  Attributes:
      long_name:  Location identifier (Grid Point ID)
      valid_min:  0
      valid_max:  3300000
#+END_EXAMPLE
:end:


#+begin_src ipython
%%time
location_avgs = output.groupby("location_id").mean("obs")
avg_ssm_groupby = location_avgs.surface_soil_moisture
avg_ssm_groupby
#+end_src

#+RESULTS:
:results:
#+BEGIN_EXAMPLE
  <xarray.DataArray 'surface_soil_moisture' (location_id: 2652)>
  dask.array<getitem, shape=(2652,), dtype=float32, chunksize=(2,), chunktype=numpy.ndarray>
  Coordinates:
    * location_id  (location_id) float64 1.163e+06 1.163e+06 ... 1.26e+06 1.26e+06
  Attributes:
      long_name:  surface soil moisture
      units:      percent saturation
      valid_min:  0
      valid_max:  10000
#+END_EXAMPLE
:end:

This takes 5.4 seconds on my own machine, which isn't too bad, but we're only working with 5 days of data here for a relatively small area. This operation could easily balloon in complexity and become intractable, especially if it needs to be repeated often. We also get these annoying warnings from flox (a package xarray uses for groupby operations) warning of slicing with an out-of-order index.

However, if we use flox directly, we can accomplish the same operation in a tiny fraction of the time (10-20 ms on my machine). I still need to investigate exactly what the difference is here.

#+begin_src ipython
%%time
import flox
avg_ssm_flox = flox.xarray.xarray_reduce(output["surface_soil_moisture"], output["location_id"], func="mean")
avg_ssm_flox
#+end_src

#+RESULTS:
:results:
#+BEGIN_EXAMPLE
  <xarray.DataArray 'surface_soil_moisture' (location_id: 2652)>
  dask.array<groupby_nanmean, shape=(2652,), dtype=float32, chunksize=(2652,), chunktype=numpy.ndarray>
  Coordinates:
    * location_id  (location_id) float64 1.163e+06 1.163e+06 ... 1.26e+06 1.26e+06
  Attributes:
      long_name:  surface soil moisture
      units:      percent saturation
      valid_min:  0
      valid_max:  10000
#+END_EXAMPLE
:end:

The time savings here seem almost too good to be true, let's check that we indeed have the same result.
#+begin_src ipython
import numpy as np
np.all(avg_ssm_groupby.values == avg_ssm_flox.values)
#+end_src

#+RESULTS:
:results:
: False
:end:

OK, that's embarrassing! The results are not identical. But maybe they're very close?

#+begin_src ipython :results output
print(
    np.nanmax(avg_ssm_groupby.values - avg_ssm_flox.values),
    np.nanmin(avg_ssm_groupby.values - avg_ssm_flox.values),
    np.nanmean(avg_ssm_groupby.values - avg_ssm_flox.values)
)
#+end_src

#+RESULTS:
:results:
1.5258789e-05 -1.5258789e-05 -3.0470326e-09
:end:

Well, it looks like the result of the direct flox calculation only varies from that of the xarray groupby operation by a very small amount, on the order of 10^-5 at most, and on the order of 10^-9 on average. I'm not sure what the cause of this discrepancy is, but it may be small enough that the time savings are worth it.

I'll step away from the data for a second and write a quick function for plotting it on a map:

#+begin_src ipython
from matplotlib import pyplot as plt
import cartopy.crs as ccrs

def simple_map(lons, lats, color_var, cmap, dates=None, cbar_label=None):
    plt.close()
    ax = plt.axes(projection=ccrs.PlateCarree())
    ax.coastlines()
    gl = ax.gridlines(draw_labels=True)
    gl.bottom_labels = False
    gl.right_labels = False
    ax.set_extent([lons.min()-5, lons.max()+5, lats.min()-5, lats.max()+5])
    # ax.set_extent([-10, 30, 35, 65])
    plt.scatter(
        lons,
        lats,
        c=color_var,
        cmap=cmap,
        s=1,
        # alpha=0.8,
        # clim=(0, 100)
    )
    if cbar_label is None:
        cbar_label = (
            f"Average {color_var.long_name}\n"
            f"({color_var.units})\n"
        )
    if dates is not None:
        cbar_label += f"\n{np.datetime_as_string(dates[0], unit='s')} - {np.datetime_as_string(dates[1], unit='s')}"

    plt.colorbar(label=(cbar_label),
                 shrink=0.5,
                 pad=0.05,
                 orientation="horizontal"
    )
    plt.tight_layout()

#+end_src

#+RESULTS:
:results:
:end:

And here is our mean soil moisture!

#+begin_src ipython
# make sure that location_ids are in the same order as our average soil moisture values
unique_lids, lid_idx = np.unique(output.location_id.values, return_index=True)
assert np.all(unique_lids == location_avgs.location_id.values)
lats = output.latitude[lid_idx]
lons = output.longitude[lid_idx]
simple_map(lons, lats, avg_ssm_flox, "viridis", (output.time.values.min(), output.time.values.max()))
#+end_src

#+RESULTS:
:results:
[[file:./obipy-resources/5cScCF.png]]
:end:


Now it's easy to make a map of any of the other variables in the dataset. Here's the average backscatter at 40 degrees incidence angle:

#+begin_src ipython
avg_sms = flox.xarray.xarray_reduce(output["surface_soil_moisture_sensitivity"], output["location_id"], func="mean")
simple_map(lons, lats, avg_sms, "viridis", (output.time.values.min(), output.time.values.max()))
#+end_src

#+RESULTS:
:results:
[[file:./obipy-resources/SKr2NC.png]]
:end:


Or we could make a timeseries plot of a variable at a single location or a collection of locations:

#+begin_src ipython
%%time
means, dts = flox.groupby_reduce(output["backscatter40"], output["time.date"], func="mean")

plt.close()
date_groups = output.groupby("time.date")
for dt, ds in date_groups:
    plt.scatter(ds["time.date"], ds.backscatter40, color="black", s=1, alpha=0.01)

plt.plot(dts, date_groups.mean().backscatter40.values, color="red")
plt.title("Daily backscatter values, Metop A, B and C")
plt.ylabel(f"{ds.backscatter40.units}")
plt.xlabel(f"date")
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))
plt.xticks(rotation=30)
plt.tight_layout()
#+end_src

#+RESULTS:
:results:
[[file:./obipy-resources/gtpYK5.png]]
:end:


We can make a 5-year climatology for our data in three lines of code, but it will take a while to run, since we'll have to read metadata from thousands of files to compile the xarray dataset. I do not recommend running this cell!


#+begin_src ipython :eval no
# five year climatology
five_years = [datetime(2015, 1, 1), datetime(2020, 1, 1)]
five_years_data = collection.read(location_id=gpis, date_range=five_years)#.load()
climatology = output.groupby("time.dayofyear").mean("obs")
#+end_src

If you need to do operations on larger chunks of time, it could be useful to convert the data to a cell file collection and work off of that. (see CellFileCollection section below)

** Converting swath collections to cell collections

To convert a collection of swath files into a collection of cell files, we only need to call a single method from our SwathFileCollection. We pass it at least an output directory path (~out_dir~), where the outputs will be written, and we can also pass it several other options.

#+begin_src ipython :eval no

# without setting this variable as False, the package will warn the user and wait for confirmation before running, since a careless use of `stack` pointing to the wrong directory could delete or ruin a lot of data.
rat.process_warnings = False
# where to save the files
cell_file_directory = ""
# a list of swath file names to write, if you have a specific list
fnames = None
# the date range to stack data from
date_range = None
# mode : "w" for creating new files if any already exist, "a" to append data to existing cell files
# note that old data and new data will not be sorted after the append
mode = "w"
# the number of processes to use when writing the data.
# does NOT have anything to do with xarray's dask processing
# I've found that using too many processes, even on machines with many cores, may not be optimal.
# A good number is 8.
processes = 8

# the maximum size of the data buffer before dumping to file (actual maximum memory used will be higher)
# default is 6144MB
buffer_memory_mb = None

collection.stack(
    output_dir=cell_file_directory,
    fnames=fnames,
    date_range=date_range,
    mode=mode,
    processes=processes,
    buffer_memory_mb=buffer_memory_mb
)
#+end_src

The output cells are in /indexed ragged array/ format. In order to convert them to /contiguous/ ragged array format, we can create a ~CellFileCollection~ from the output directory, and call the method ~to_contiguous()~:

#+begin_src ipython :eval no
cell_collection = rat.CellFileCollection.from_product_id(cell_file_directory, product_id="H121_v1.0")
contiguous_cell_file_directory = ""
cell_collection.to_contiguous(contiguous_cell_file_directory)
#+end_src

This will sort the entire dataset first by time and then by ~locationIndex~, and then replace the dataset's ~locationIndex~ variable with a ~row_size~ variable. At this point it is no longer practically possible to append new data to the dataset without first re-converting it to indexed ragged array format and then converting back.


* Working with collections of cell files
Right now, although ~CellFileCollection~ exists, it currently is optimized for use under the hood of ~CellFileCollectionStack~. Ideally both could be used by users, but there are still some bugs to be worked out and some refactoring to do. To work with a single collection of cell files, simply create a ~CellFileCollectionStack~ with a single collection inside.

* Working with stacks of cell file collections

** Creating a cell file collection

#+begin_src ipython
from datetime import datetime
from importlib import reload
from time import time

import ascat.read_native.ragged_array_ts as rat
reload(rat)

#+end_src

#+RESULTS:
:results:
: <module 'ascat.read_native.ragged_array_ts' from '/home/charriso/Projects/ascat/src/ascat/read_native/ragged_array_ts.py'>
:end:

Our cell files, in this case, all live in a single directory, so that's the path we'll pass to ~rat.CellFileCollectionStack.from_product_id()~. If we had multiple sets of cell files contained in different directories, we could pass a list of these directories' paths, assuming they were all of the same product type (and therefore had the same dimensions, data variables, etc).

The product id, ~"H121_V1.0"~, refers to a specific handler class defined in ~ascat.read_native.xarray_io~. There are several of these already defined for various products we use, and it is also possible to define your own handler class if you need to process a product we haven't included in this package already.

#+begin_src ipython
cell_source = "/home/charriso/p14/data-write/USERS/charriso/h121_merged/metop_abc/"
collection = rat.CellFileCollectionStack.from_product_id(cell_source, "H121_V1.0")
#+end_src

#+RESULTS:
:results:
:end:

** Reading from a cell file collection
We can read data from a specific geographic and temporal extent, but if you have a single collection, it may actually take longer to create an xarray dataset if you try to trim down the time range. In this case it is best to only subset by geographic extent on read, and then do any temporal subsetting after the xarray dataset is created, but before the data is actually loaded into memory with ~.load()~.

On the other hand, if you have a stack with multiple collections that cover different time ranges, you can possibly save a lot of time when reading using temporal subsetting. (Imagine you have dozens of weekly cell collections and only need two weeks - no need to even look at the other files).

Our options for geographic extent are ~cell~, ~bbox~, ~geom~, and ~location_id~. ~cell~ is a list of cell indices, ~bbox~ is a tuple of (latmin, latmax, lonmin, lonmax), ~geom~ is a shapely geometry object, and ~location_id~ is a list of location indices.

#+begin_src ipython
bounds = (43, 51, 11, 21) #latmin, latmax, lonmin, lonmax
dates = (np.datetime64(datetime(2020, 12, 1)), np.datetime64(datetime(2020, 12, 15)))
#+end_src

#+RESULTS:
:results:
:end:


#+begin_src ipython
output_bbox = collection.read(bbox=bounds)#, date_range=dates)
output_bbox
#+end_src

#+RESULTS:
:results:
#+BEGIN_EXAMPLE
  <xarray.Dataset>
  Dimensions:                            (obs: 59143813, locations: 4378)
  Coordinates:
      time                               (obs) datetime64[ns] 2007-01-01T08:00:...
      lon                                (locations) float32 ...
      lat                                (locations) float32 ...
      alt                                (locations) float32 ...
  Dimensions without coordinates: obs, locations
  Data variables: (12/22)
      locationIndex                      (obs) int64 ...
      as_des_pass                        (obs) float32 ...
      swath_indicator                    (obs) float32 ...
      surface_soil_moisture              (obs) float32 ...
      surface_soil_moisture_noise        (obs) float32 ...
      backscatter40                      (obs) float32 ...
      ...                                 ...
      topographic_complexity             (obs) float32 ...
      subsurface_scattering_probability  (obs) float32 ...
      global_attributes_flag             (locations) int64 ...
      sat_id                             (obs) float32 ...
      location_id                        (locations) int64 ...
      location_description               (locations) object ...
  Attributes: (12/15)
      title:             ASCAT surface soil moisture near real-time product
      summary:           ASCAT surface soil moisture expressed in degree of sat...
      doi:               unset
      keywords:          Metop-A ASCAT surface soil moisture
      history:           original generated product
      institution:       H SAF
      ...                ...
      disposition_mode:  Operational
      environment:       Operational
      references:        h-saf.eumetsat.int
      software_version:  warp_h_nrt 0.0.0
      conventions:       CF-1.10
      featureType:       timeSeries
#+END_EXAMPLE
:end:

Now that we've read the dataset for our geographic area, we can create a subset for our area of temporal interest:

#+begin_src ipython
date_range_data = output_bbox.sel(obs=(output_bbox["time"] > dates[0]) & (output_bbox["time"] < dates[1]))
date_range_data
#+end_src

#+RESULTS:
:results:
#+BEGIN_EXAMPLE
  <xarray.Dataset>
  Dimensions:                            (obs: 226965, locations: 4378)
  Coordinates:
      time                               (obs) datetime64[ns] 2020-12-01T08:11:...
      lon                                (locations) float32 ...
      lat                                (locations) float32 ...
      alt                                (locations) float32 ...
  Dimensions without coordinates: obs, locations
  Data variables: (12/22)
      locationIndex                      (obs) int64 ...
      as_des_pass                        (obs) float32 ...
      swath_indicator                    (obs) float32 ...
      surface_soil_moisture              (obs) float32 ...
      surface_soil_moisture_noise        (obs) float32 ...
      backscatter40                      (obs) float32 ...
      ...                                 ...
      topographic_complexity             (obs) float32 ...
      subsurface_scattering_probability  (obs) float32 ...
      global_attributes_flag             (locations) int64 ...
      sat_id                             (obs) float32 ...
      location_id                        (locations) int64 ...
      location_description               (locations) object ...
  Attributes: (12/15)
      title:             ASCAT surface soil moisture near real-time product
      summary:           ASCAT surface soil moisture expressed in degree of sat...
      doi:               unset
      keywords:          Metop-A ASCAT surface soil moisture
      history:           original generated product
      institution:       H SAF
      ...                ...
      disposition_mode:  Operational
      environment:       Operational
      references:        h-saf.eumetsat.int
      software_version:  warp_h_nrt 0.0.0
      conventions:       CF-1.10
      featureType:       timeSeries
#+END_EXAMPLE
:end:

Now let's map the average surface soil moisture over the area and time range we selected.

#+begin_src ipython
avg_sm = flox.xarray.xarray_reduce(date_range_data["surface_soil_moisture"], date_range_data["locationIndex"], func="mean")
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src ipython
import numpy as np
lons = date_range_data.lon.values[avg_sm.locationIndex.values]
lats = date_range_data.lat.values[avg_sm.locationIndex.values]
simple_map(lons, lats, avg_sm, "Greens", (date_range_data.time.values.min(), date_range_data.time.values.max()))
#+end_src

#+RESULTS:
:results:
[[file:./obipy-resources/QPgIbI.png]]
:end:


When we read data using cell ids, the process is just as easy:

#+begin_src ipython
output_cells = collection.read(cell=[1431, 1432, 1395, 1396])#, date_range=dates)
#+end_src

#+RESULTS:
:results:
:end:

#+begin_src ipython
avg_sm = flox.xarray.xarray_reduce(output_cells["surface_soil_moisture"], output_cells["locationIndex"], func="mean")
lons = output_cells.lon.values[avg_sm.locationIndex.values]
lats = output_cells.lat.values[avg_sm.locationIndex.values]
simple_map(lons, lats, avg_sm, "Greens", (output_cells.time.values.min(), output_cells.time.values.max()))
#+end_src

#+RESULTS:
:results:
[[file:./obipy-resources/QhyfT0.png]]
:end:

Oops, I accidentally forgot to filter by time range, but it took flox only a few seconds to calculate the average surface soil moisture over the entire time range of the dataset for these cells!

*** Using geometries

If you have a shapefile you would like to use to filter your data, you will have to turn it into a shapely geometry object. There are a few ways you could do this (using geopandas, fiona, or ogr, for example). This function uses cartopy's shapereader to fetch a world country boundaries shapefile from Natural Earth, and then uses shapely to create a geometry object from the desired country names.

#+begin_src ipython
import cartopy.io.shapereader as shpreader
from shapely.ops import unary_union

def get_country_geometries(country_names, resolution="10m", ne_product="admin_0_countries"):
    countries = shpreader.Reader(
        shpreader.natural_earth(
            resolution=resolution,
            category="cultural",
            name=ne_product,
        )
    ).records()
    if isinstance(country_names, str):
        country_names = [country_names]
    for i in range(len(country_names)):
        country_names[i] = country_names[i].lower()

    geometries = []
    desired_shp = None
    for loop_country in countries:
        if loop_country.attributes["SOVEREIGNT"].lower() in country_names:
            desired_shp = loop_country.geometry
            if desired_shp is not None:
                geometries.append(desired_shp)
    return unary_union(geometries)
#+end_src

#+RESULTS:
:results:
:end:

If we are interested in the Baltic countries, for example, we can simply pass a list of their names to ~get_country_geometries~, then pass the resulting geometry to the ~geom~ argument of ~collection.read()~.

#+begin_src ipython
baltics = ["Estonia", "Latvia", "Lithuania"]
country_data = collection.read(geom=get_country_geometries(baltics))
#+end_src

#+RESULTS:
:results:
:end:

With the magic of flox, groupby operations are fast and easy. Here we calculate the average summer soil moisture for each location in the Baltics across the entire time range of the dataset.

#+begin_src ipython
import numpy as np
from flox.xarray import xarray_reduce

baltic_summer = country_data.sel(obs=(country_data.time.dt.season == "JJA"))
avg_sm = xarray_reduce(baltic_summer["surface_soil_moisture"], baltic_summer["locationIndex"], func="mean")
lons = country_data.lon.values[avg_sm.locationIndex.values]
lats = country_data.lat.values[avg_sm.locationIndex.values]
label = (
        f"Average summer soil moisture "
        f"in the Baltic countries\n"
        f"({avg_sm.units})\n"
        f"June, July, and August of 2007 - 2022"
)
simple_map(lons, lats, avg_sm, "Greens", cbar_label=label)
#+end_src

#+RESULTS:
:results:
[[file:./obipy-resources/qhFP8Y.png]]
:end:

Remember that climatology we were going to make in the swaths section? Let's do that now, for all the variables in the dataset.

#+begin_src ipython
# 15-year climatology
climatology = xarray_reduce(country_data, country_data["time"].dt.dayofyear, func="mean")
climatology
#+end_src

On my machine that only took 18 seconds.
